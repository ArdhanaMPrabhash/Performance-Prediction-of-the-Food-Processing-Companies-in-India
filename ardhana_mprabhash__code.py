# -*- coding: utf-8 -*-
"""Ardhana_MPrabhash_ code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a0MvCnfvPli0WAhz2lJ3lIwA7imFtz63
"""

#Preprocessing the data using various imputation methods

#median imputation
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
df_train=pd.read_csv('C:/Users/Ardhana/Desktop/train_data.csv')
df_train.head()
df_train.info()

missing_value_dftrain = df_train.isnull()
print(missing_value_dftrain)

missing_counttrain =missing_value_dftrain.sum()
print(missing_counttrain)
df_train.describe()

dfmed =df1.fillna(df1.median())
dfmed

label=pd.read_csv('C:/Users/Ardhana/Desktop/targets.csv',header=None)
label.columns = ['target']
label

merged_dfmed = pd.concat([dfmed,label], axis=1)
merged_dfmed

import pandas as pd

merged_dfmed.to_csv("DEVMED.csv", index=False)
from IPython.display import FileLink
FileLink("DEVMED.csv")

#mean imputation
df_train
df2 = df_train.drop(['Company Name'], axis =1)
df2
missing_value_dftrain = df_train.isnull()
print(missing_value_dftrain)
missing_counttrain =missing_value_dftrain.sum()
print(missing_counttrain)
df_train.describe()
dfmean =df1.fillna(df1.mean())
dfmean
label=pd.read_csv('C:/Users/Ardhana/Desktop/targets.csv',header=None)
label.columns = ['target']
label
merged_dfmean = pd.concat([dfmean,label], axis=1)
merged_dfmean
merged_dfmean.to_csv("DEVMEAN.csv", index=False)
from IPython.display import FileLink
FileLink("DEVMEAN.csv")

#Interpolation method
import pandas as pd

# Load your CSV file into a pandas DataFrame
#df = pd.read_csv('your_data.csv')

# Perform linear interpolation along columns
df_interpolated = df_train.interpolate(method='linear', axis=0)
print(df_interpolated)
# Save the interpolated DataFrame back to a CSV file
df_interpolated.to_csv('interpolated_data.csv', index=False)
df_interpolated
df_interpolated.describe()
#missing values
missing_value_int=df_interpolated.isnull()
print(missing_value_int)
missing_int_count = missing_value_int.sum()
print(missing_int_count)
label1=pd.read_csv('C:/Users/Ardhana/Desktop/targets.csv',header=None)
label1.columns = ['target']
label1
merged_dfint = pd.concat([df_interpolated,label1], axis=1)
merged_dfint

merged_dfint1= merged_dfint.drop(['Company Name', 'Year'], axis =1)
merged_dfint1
import pandas as pd

merged_dfint1.to_csv("interpolatedata.csv", index=False)
from IPython.display import FileLink
FileLink("interpolatedata.csv")

#visualization

df_train.hist(bins = 20, figsize = (10, 10))

import pandas as pd
df=pd.read_csv("C:\\Users\\Ardhana\\Downloads\\DEVMED.csv")
df.head()
df.info()
df.describe()
df.median()
df.mean()
plt.figure(figsize=(20,20))
sns.heatmap(df.corr(),annot=True)
plt.show()

# import modules
import matplotlib.pyplot as mp
import pandas as pd
import seaborn as sb

# load data
data = pd.read_csv("mediandata.csv")

# plotting heatmap
sb.heatmap(data.corr(), annot=None, cbar_kws={'shrink': 0.6})

# displaying heatmap
mp.show()

# Regression models:

import csv,os,re,sys,codecs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib,  statistics
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.feature_selection import SelectKBest,chi2
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import classification_report
from collections import Counter


#path='\\C:\\Users\\Ardhana\\Downloads\\'

"""
#Load the file using CSV Reader
fl=open(self.path+'winequality_white.csv',"r")
reader = list(csv.reader(fl,delimiter='\n'))
fl.close()
data=[]; labels=[];
for item in reader[1:]:
    item=''.join(item).split(';')
   labels.append(item[-1])
    data.append(item[:-1])
 labels=[int(''.join(item)) for item in labels]
data=np.asarray(data)
"""

# Load the file using Pandas
reader=pd.read_csv('C:\\Users\\Ardhana\\Downloads\\DEVMED.csv')
print(reader)



# Select all rows except the ones belong to particular class'
# mask = reader['target'] == 9
# reader = reader[~mask]

data=reader.iloc[:, :-1]
labels=reader['target']


# Training and test split WITHOUT stratification
training_data, validation_data, training_cat, validation_cat = train_test_split(data, labels,
                                                test_size=0.10, random_state=42)

print('\n Training Data ')
training_cat=[x for x in training_cat]

print('\n Validation Data ')
validation_cat=[x for x in validation_cat]

  # Classification

rgr1 = LinearRegression()
rgr2 = Ridge(alpha=1.0,solver='lbfgs',positive=True)
rgr3 = Lasso(alpha=1.0)
rgr4 = DecisionTreeRegressor(random_state=0)


rgr4.fit(training_data,training_cat)
predicted=rgr4.predict(validation_data)

# Regression report
mse=mean_squared_error(validation_cat,predicted,squared=True)
print ('\n MSE:\t'+str(mse))
rmse=mean_squared_error(validation_cat,predicted,squared=False)
print ('\n RMSE:\t'+str(rmse))
r2=r2_score(validation_cat,predicted,multioutput='variance_weighted')
print ('\n R2-Score:\t'+str(r2))

import csv,os,re,sys,codecs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib,  statistics
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest,chi2
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from collections import Counter


class regression():
     def __init__(self,path='C:\\Users\\Ardhana\\Downloads\\DEVMED.csv',rgr_opt='lr',no_of_selected_features=None):
        self.path = path
        self.rgr_opt=rgr_opt
        self.no_of_selected_features=no_of_selected_features
        if self.no_of_selected_features!=None:
            self.no_of_selected_features=int(self.no_of_selected_features)

# Selection of regression techniques
     def regression_pipeline(self):
    # AdaBoost
        if self.rgr_opt=='ab':
            print('\n\t### AdaBoost Regression ### \n')
            be1 = LinearRegression()
            be2 = DecisionTreeRegressor(random_state=0)
            be2 = Ridge(alpha=1.0,solver='lbfgs',positive=True)
            rgr = AdaBoostRegressor(n_estimators=100)
            rgr_parameters = {
            'rgr__base_estimator':(be1,be2),
            'rgr__random_state':(0,10),
            }
    # Decision Tree
        elif self.rgr_opt=='dt':
            print('\n\t### Decision Tree ### \n')
            rgr = DecisionTreeRegressor(random_state=40)
            rgr_parameters = {
            'rgr__criterion':('squared_error','friedman_mse','poisson'),
            'rgr__max_features':(None,'auto', 'sqrt', 'log2'),
            'rgr__max_depth':(10,40,45,60),
            'rgr__ccp_alpha':(0.009,0.01,0.05,0.1),
            }
    # Ridge Regression
        elif self.rgr_opt=='rg':
            print('\n\t### Ridge Regression ### \n')
            rgr = Ridge(alpha=1.0,positive=True)
            rgr_parameters = {
            'rgr__solver':('auto', 'lbfgs'),
            }
    # Linear Regression
        elif self.rgr_opt=='lr':
            print('\n\t### Linear Regression ### \n')
            rgr = LinearRegression()
            rgr_parameters = {
            'rgr__positive':(True,False),
            }
    # Random Forest
        elif self.rgr_opt=='rf':
            print('\n\t ### Random Forest ### \n')
            rgr = RandomForestRegressor(max_features=None)
            rgr_parameters = {
            'rgr__criterion':('squared_error','friedman_mse','poisson'),
            'rgr__n_estimators':(30,50,100),
            'rgr__max_depth':(10,20,30),
            'rgr__max_features':(None,'auto', 'sqrt', 'log2'),
            }
    # Support Vector Machine
        elif self.rgr_opt=='svr':
            print('\n\t### SVM Regressor ### \n')
            rgr = SVR(probability=True)
            rgr_parameters = {
            'rgr__C':(0.1,1,100),
            'rgr__kernel':('linear','rbf','poly','sigmoid'),
            }
        else:
            print('Select a valid classifier \n')
            sys.exit(0)
        return rgr,rgr_parameters

# Load the data
     def get_data(self):
    # Load the file using CSV Reader
        # fl=open(self.path+'winequality_white.csv',"r")
        # reader = list(csv.reader(fl,delimiter='\n'))
        # fl.close()
        # data=[]; labels=[];
        # for item in reader[1:]:
        #     item=''.join(item).split(';')
        #     labels.append(item[-1])
        #     data.append(item[:-1])
        # # labels=[int(''.join(item)) for item in labels]
        # data=np.asarray(data)

    # Load the file using Pandas
        reader=pd.read_csv(self.path)

    # Select all rows except the ones belong to particular class'
        # mask = reader['class'] == 9
        # reader = reader[~mask]

        data=reader.iloc[:, :-1]
        labels=reader['target']

        # Training and Test Split
        training_data, validation_data, training_cat, validation_cat = train_test_split(data, labels,
                                               test_size=0.3, random_state=42)

        return training_data, validation_data, training_cat, validation_cat

# Regression using the Gold Statndard after creating it from the raw text
     def regression(self):
   # Get the data
        training_data, validation_data, training_cat, validation_cat=self.get_data()

        rgr,rgr_parameters=self.regression_pipeline()
        pipeline = Pipeline([('rgr', rgr),])
        grid = GridSearchCV(pipeline,rgr_parameters,scoring='f1_macro',cv=10)
        grid.fit(training_data,training_cat)
        rgr= grid.best_estimator_
        print('\n\n The best set of parameters of the pipiline are: ')
        print(rgr)
        joblib.dump(rgr, self.path+'training_modelmed.joblib')
        predicted=rgr.predict(validation_data)


    # Regression report
        mse=mean_squared_error(validation_cat,predicted,squared=True)
        print ('\n MSE:\t'+str(mse))
        rmse=mean_squared_error(validation_cat,predicted,squared=False)
        print ('\n RMSE:\t'+str(rmse))
        r2=r2_score(validation_cat,predicted,multioutput='variance_weighted')
        print ('\n R2-Score:\t'+str(r2))

import csv,os,re,sys,codecs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib,  statistics
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest,chi2
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from collections import Counter


class regression():
     def __init__(self,path='C:\\Users\\Ardhana\\Downloads\\DEVMED.csv',rgr_opt='lr',no_of_selected_features=None):
        self.path = path
        self.rgr_opt=rgr_opt
        self.no_of_selected_features=no_of_selected_features
        if self.no_of_selected_features!=None:
            self.no_of_selected_features=int(self.no_of_selected_features)

# Selection of regression techniques
     def regression_pipeline(self):
    # AdaBoost
        if self.rgr_opt=='ab':
            print('\n\t### AdaBoost Regression ### \n')
            be1 = LinearRegression()
            be2 = DecisionTreeRegressor(random_state=0)
            be2 = Ridge(alpha=1.0,solver='lbfgs',positive=True)
            rgr = AdaBoostRegressor(n_estimators=100)
            rgr_parameters = {
            'rgr__base_estimator':(be1,be2),
            'rgr__random_state':(0,10),
            }
    # Decision Tree
        elif self.rgr_opt=='dt':
            print('\n\t### Decision Tree ### \n')
            rgr = DecisionTreeRegressor(random_state=40)
            rgr_parameters = {
            'rgr__criterion':('squared_error','friedman_mse','poisson'),
            'rgr__max_features':(None,'auto', 'sqrt', 'log2'),
            'rgr__max_depth':(10,40,45,60),
            'rgr__ccp_alpha':(0.009,0.01,0.05,0.1),
            }
    # Ridge Regression
        elif self.rgr_opt=='rg':
            print('\n\t### Ridge Regression ### \n')
            rgr = Ridge(alpha=1.0,positive=True)
            rgr_parameters = {
            'rgr__solver':('auto', 'lbfgs'),
            }
    # Linear Regression
        elif self.rgr_opt=='lr':
            print('\n\t### Linear Regression ### \n')
            rgr = LinearRegression()
            rgr_parameters = {
            'rgr__positive':(True,False),
            }
    # Random Forest
        elif self.rgr_opt=='rf':
            print('\n\t ### Random Forest ### \n')
            rgr = RandomForestRegressor(max_features=None)
            rgr_parameters = {
            'rgr__criterion':('squared_error','friedman_mse','poisson'),
            'rgr__n_estimators':(30,50,100),
            'rgr__max_depth':(10,20,30),
            'rgr__max_features':(None,'auto', 'sqrt', 'log2'),
            }
    # Support Vector Machine
        elif self.rgr_opt=='svr':
            print('\n\t### SVM Regressor ### \n')
            rgr = SVR(probability=True)
            rgr_parameters = {
            'rgr__C':(0.1,1,100),
            'rgr__kernel':('linear','rbf','poly','sigmoid'),
            }
        else:
            print('Select a valid classifier \n')
            sys.exit(0)
        return rgr,rgr_parameters

# Load the data
     def get_data(self,filename):
    # Load the file using CSV Reader
        # fl=open(self.path+'winequality_white.csv',"r")
        # reader = list(csv.reader(fl,delimiter='\n'))
        # fl.close()
        # data=[]; labels=[];
        # for item in reader[1:]:
        #     item=''.join(item).split(';')
        #     labels.append(item[-1])
        #     data.append(item[:-1])
        # # labels=[int(''.join(item)) for item in labels]
        # data=np.asarray(data)

    # Load the file using Pandas
        reader=pd.read_csv(self.path+filename)

    # Select all rows except the ones belong to particular class'
        # mask = reader['class'] == 9
        # reader = reader[~mask]

        data=reader.iloc[:, :-1]
        labels=reader['target']

        return data, labels

# Regression using the Gold Statndard after creating it from the raw text
     def regression(self):
   # Get the data
        data,labels=self.get_data('')
        data=np.asarray(data)

# Experiments using training data only during training phase (dividing it into training and validation set)
        skf = KFold(n_splits=10)
        predicted_target=[]; actual_target=[];
        count=0;
        for train_index, test_index in skf.split(data,labels):
            X_train=[]; y_train=[]; X_test=[]; y_test=[]
            for item in train_index:
                X_train.append(data[item])
                y_train.append(labels[item])
            for item in test_index:
                X_test.append(data[item])
                y_test.append(labels[item])
            count+=1
            print('Training Phase '+str(count))
            rgr,rgr_parameters=self.regression_pipeline()
            pipeline = Pipeline([('rgr', rgr),])
            grid = GridSearchCV(pipeline,rgr_parameters,scoring='f1_micro',cv=10)
            grid.fit(X_train,y_train)
            rgr= grid.best_estimator_
            # print('\n\n The best set of parameters of the pipiline are: ')
            # print(rgr)
            predicted=rgr.predict(X_test)
            for item in y_test:
                actual_target.append(item)
            for item in predicted:
                predicted_target.append(item)

    # Regression report
        print('\n\n Performance on Training Data \n')
        mse=mean_squared_error(actual_target,predicted_target,squared=True)
        print ('\n MSE:\t'+str(mse))
        rmse=mean_squared_error(actual_target,predicted_target,squared=False)
        print ('\n RMSE:\t'+str(rmse))
        r2=r2_score(actual_target,predicted_target,multioutput='variance_weighted')
        print ('\n R2-Score:\t'+str(r2))

        # Experiments on Given Test Data during Test Phase

        tst_data,tst_target=self.get_data('')
        tst_data=np.asarray(data)
        predicted=rgr.predict(tst_data)

        print('\n\n Performance on Test Data \n')
        mse=mean_squared_error(tst_target,predicted_target,squared=True)
        print ('\n MSE:\t'+str(mse))
        rmse=mean_squared_error(tst_target,predicted_target,squared=False)
        print ('\n RMSE:\t'+str(rmse))
        r2=r2_score(tst_target,predicted_target,multioutput='variance_weighted')
        print ('\n R2-Score:\t'+str(r2))

#main regression

print('regression 2 median with testsize 30')
from regression2medsize2 import regression

#from regression3 import regression

import warnings
warnings.filterwarnings("ignore")


rgr=regression('C:\\Users\\Ardhana\\Downloads\\DEVMED.csv', rgr_opt='rf',
                        no_of_selected_features=4)

rgr.regression()

from sklearn.ensemble import RandomForestRegressor

# Load your data into a pandas DataFrame
df = pd.read_csv('C:\\Users\\Ardhana\\Downloads\\DEVMED.csv')

# Separate features and target variable
X = df.drop('target', axis=1)
y = df['target']

# Fit a Random Forest Regressor model
model = RandomForestRegressor()
model.fit(X, y)

# Get feature importances
feature_importances = model.feature_importances_

# Select top features based on importance
num_features = 8
top_features_indices = np.argsort(feature_importances)[::-1][:num_features]
X_selected = X.iloc[:, top_features_indices]


import pandas as pd
import numpy as np
X_selected.to_csv("x_selected.csv", index=False)
from IPython.display import FileLink
FileLink("x_selected.csv")

#  Predicting the test label for the given data using trained model

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
df_test=pd.read_csv('C:\\Users\\Ardhana\\Downloads\\test_data.csv')

df_test

df_test.info()

missing_value_dftest= df_test.isnull()
print(missing_value_dftest)

missing_counttest =missing_value_dftest.sum()
print(missing_counttest)

df_test.describe()

df1 = df_test.drop(['Company Name', 'Year'], axis =1)
df1

df1.mean()

df1.median()

dfmed =df1.fillna(df1.median())
dfmed

#prediction using median imputed test data ....Random forest regressor

#USING TRAINED MODEL PREDICTING THE TEST LABELS

from joblib import load

# Load the trained model
model = load('C:\\Users\\Ardhana\\Downloads\\DEVMED.csvtraining_modelmed.joblib')
model

# Make predictions on the test data
predictions2 = model.predict(dfmed)
predictions2

#convert numpy array to dataframe

predicted_y = pd.DataFrame(predictions2)
print("\nPandas DataFrame: ")
predicted_y

import pandas as pd

predicted_y.to_csv("Predicted labels.csv", index=False)
from IPython.display import FileLink
FileLink("Predicted labels.csv")